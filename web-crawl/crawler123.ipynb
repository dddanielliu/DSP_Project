{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "211e2fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling URL: https://law.moj.gov.tw/LawClass/LawAll.aspx?PCODE=N0060010\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote\n",
    "import pandas as pd\n",
    "database=pd.DataFrame([],columns=[\"actname\",\"title\",\"article\"])\n",
    "\n",
    "#%%第二段\n",
    "#urls=[]\n",
    "# url = input('請輸入全國法規資料庫網址：')\n",
    "\n",
    "#urls=urls[0:6] \n",
    "#start_time=time.time()\n",
    "#print(start_time)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re  # 導入正規表達式模組\n",
    "\n",
    "def crawl_questions(url, filename):\n",
    "    web = requests.get(url)\n",
    "    soup = BeautifulSoup(web.text, \"html.parser\")\n",
    "    \n",
    "    # 找出所有 class='row' 的 div (條文) 和 class='char-2' 的 div (章節標題)\n",
    "    major_elements = soup.find_all('div', class_=['row', 'char-2']) \n",
    "    \n",
    "    lawbase = pd.DataFrame([], columns=[\"actname\", \"chapter\", \"title\", \"article\"])\n",
    "    current_chapter = \"\"  # 初始化章節名稱\n",
    "\n",
    "    for element in major_elements:\n",
    "        \n",
    "        # 檢查是否為章節標題容器 (<div class=\"char-2\">)\n",
    "        if 'char-2' in element.get('class', []):\n",
    "            \n",
    "            chapter_text = element.text.strip()\n",
    "            \n",
    "            # --- 關鍵正規表達式清理步驟 ---\n",
    "            # 1. 將 \"第\" 後面到 \"章\" 前面的所有空白字元 (\\s 是所有空白字元的簡寫) 替換為空字串。\n",
    "            # 2. 如果你的空白是 \\xa0 (非中斷空格)，\\s 也通常能涵蓋。\n",
    "            #    如果你想更精確地匹配，可以使用 [\\s\\xa0]\n",
    "            # 模式：(第)(空白字元+)(章) -> 替換成 \\1\\3 (即：第 + 章)\n",
    "            cleaned_chapter_text = re.sub(\n",
    "                r\"第\\s*(.*?)\\s*章\",                      # 匹配「第 ... 章」\n",
    "                lambda m: \"第\" + m.group(1).replace(\" \", \"\") + \"章\",  # 去除中間空白\n",
    "                chapter_text,\n",
    "                count=1                                 # 只處理第一個匹配\n",
    "            )\n",
    "            \n",
    "            current_chapter = cleaned_chapter_text\n",
    "            \n",
    "        # 檢查是否為條文容器 (<div class=\"row\">)\n",
    "        elif 'row' in element.get('class', []):\n",
    "            \n",
    "            title_div = element.find('div', class_='col-no')\n",
    "            article_div = element.find('div', class_='law-article')\n",
    "            \n",
    "            if title_div and article_div:\n",
    "                exports = pd.DataFrame()\n",
    "                \n",
    "                title_text = title_div.text.replace(\"本條文有附件\", \"\").replace(\" \", \"\").strip()\n",
    "                \n",
    "                exports[\"actname\"] = [filename]\n",
    "                # 使用已經清理過的 chapter \n",
    "                exports[\"chapter\"] = [current_chapter] \n",
    "                exports[\"title\"] = [title_text]\n",
    "                exports[\"article\"] = [article_div.text.strip()]\n",
    "                \n",
    "                lawbase = pd.concat([lawbase, exports], ignore_index=True)\n",
    "                \n",
    "    return lawbase\n",
    "\n",
    "url = \"https://law.moj.gov.tw/LawClass/LawAll.aspx?PCODE=N0060010\"\n",
    "database=pd.DataFrame([],columns=[\"actname\",\"title\",\"article\"])\n",
    "print(\"Crawling URL:\", url)\n",
    "# Call the function with the URL\n",
    "web = requests.get(url)\n",
    "soup = BeautifulSoup(web.text, \"html.parser\")\n",
    "filename=soup.find('table').find('a').text\n",
    "lawbase=crawl_questions(url, filename)\n",
    "database=pd.concat([lawbase,database])\n",
    "#database.to_csv(\"{}.csv\".format(filename))\n",
    "#%%第三段\n",
    "database.to_csv(\"{}_{}.csv\".format(filename, url.replace(\":\", \"_\").replace(\"/\", \"_\").replace(\"?\", \"_\")),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe67c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
